{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f081508",
   "metadata": {},
   "source": [
    "<h1>\"Pre-lecture\" HW [completion prior to next LEC is suggested but not mandatory]<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2573555",
   "metadata": {},
   "source": [
    "<h3>1. Pick one of the datasets from the ChatBot session(s) of the TUT demo (or from your own ChatBot session if you wish) and use the code produced through the ChatBot interactions to import the data and confirm that the dataset has missing values<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605e2e42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIPS                   746\n",
      "Admin2                 741\n",
      "Province_State         178\n",
      "Country_Region           0\n",
      "Last_Update              0\n",
      "Lat                     89\n",
      "Long_                   89\n",
      "Confirmed                0\n",
      "Deaths                   0\n",
      "Recovered                0\n",
      "Active                   0\n",
      "Combined_Key             0\n",
      "Incident_Rate           89\n",
      "Case_Fatality_Ratio     48\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_daily_reports/01-01-2021.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "missing_values = df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e5fa3",
   "metadata": {},
   "source": [
    "**Summary of Interaction:**\n",
    "\n",
    "Initial Request for Dataset:\n",
    "\n",
    "The user requested a dataset without specifying particular criteria.\n",
    "Dataset Suggestions:\n",
    "\n",
    "I provided several widely used datasets, including:\n",
    "Iris Dataset: For classification tasks.\n",
    "COVID-19 Data: Comprehensive global COVID-19 statistics.\n",
    "World Happiness Report: Annual data on global happiness scores.\n",
    "Titanic Dataset: Passenger data from the Titanic, commonly used for classification.\n",
    "Global Terrorism Database: Information on terrorist events worldwide.\n",
    "Python Code Request:\n",
    "\n",
    "The user asked for Python code to check for missing values in a specific dataset located at csse_covid_19_data/csse_covid_19_daily_reports/01-01-2021.csv.\n",
    "Python Code Provided:\n",
    "\n",
    "I provided a Python code snippet using pandas to load the dataset and check for missing values. The code:\n",
    "Loads the dataset from the provided URL.\n",
    "Displays basic information about the dataset.\n",
    "Checks for missing values and lists columns with missing values.\n",
    "Difference Between isna() and isnull():\n",
    "\n",
    "The user asked about the difference between isna() and isnull() methods in pandas.\n",
    "I explained that both methods are equivalent and can be used interchangeably to detect missing values. They return a DataFrame or Series of the same shape with True for missing values and False for non-missing values. I provided a brief example to illustrate their use.   \n",
    "\n",
    "https://chatgpt.com/share/e97c2d31-9fbd-4e73-9202-2b5e68759cc5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2740d",
   "metadata": {},
   "source": [
    "________________________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc2347",
   "metadata": {},
   "source": [
    "<h3>2. Start a new ChatBot session with an initial prompt introducing the dataset you're using and request help to determine how many columns and rows of data a pandas DataFrame has, and then<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3494cee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataset: (4011, 14)\n"
     ]
    }
   ],
   "source": [
    "# Import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_daily_reports/01-01-2021.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display the dimensions of the dataset\n",
    "print(\"Dimensions of the dataset:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5e1a7",
   "metadata": {},
   "source": [
    "**2.2** Observations refer to the individual data entries or records in a dataset. In the context of the COVID-19 dataset that I use, each observation represents data collected for a specific location (such as a country, state, or province) on a particular date. For example, one observation include the total number of COVID-19 cases reported in a state on a specific day. Variables are the distinct types of information or attributes collected for each observation. They are represented by the columns in a dataset. In the dataset that I use, the variables include attributes such Province/State, Country/Region, Last Update, Confirmed, Deaths, Recovered. In summary, observations are the individual records (rows) in the dataset, and variables are the different pieces of information collected (columns) for each record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bc62b",
   "metadata": {},
   "source": [
    "**Summary of Interactions:**\n",
    "\n",
    "User:\n",
    "The user downloaded a dataset from the provided URL and wanted to determine the size (dimensions) of the dataset using Python in a Jupyter notebook.\n",
    "\n",
    "Assistant:\n",
    "\n",
    "Instructions Provided:\n",
    "\n",
    "Import Libraries: Use the pandas library.\n",
    "Load Dataset: Use pd.read_csv(url) to load the dataset from the given URL.\n",
    "Check Dimensions: Use data.shape to find the dimensions of the dataset.\n",
    "Example code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_daily_reports/01-01-2021.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "print(\"Dimensions of the dataset:\", data.shape)\n",
    "Explanation of Terms:\n",
    "\n",
    "Observations: Each row in the dataset represents an observation. In this case, it typically refers to data for a specific location and date.\n",
    "Variables: Each column in the dataset represents a variable. Variables include columns such as Province/State, Country/Region, Last Update, Confirmed, Deaths, and Recovered.\n",
    "\n",
    "https://chatgpt.com/share/6b594c4f-cf68-4f2b-9cfd-66e2ee23722e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1c251",
   "metadata": {},
   "source": [
    "<h3>3. Ask the ChatBot how you can provide simple summaries of the columns in the dataset and use the suggested code to provide these summaries for your dataset<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d6359c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               FIPS          Lat        Long_     Confirmed        Deaths  \\\n",
      "count   3265.000000  3922.000000  3922.000000  4.011000e+03   4011.000000   \n",
      "mean   32386.688208    35.768908   -71.113714  2.102613e+04    476.518823   \n",
      "std    18017.294540    13.391261    55.220345  1.055172e+05   2833.090464   \n",
      "min       66.000000   -71.949900  -175.198200  0.000000e+00      0.000000   \n",
      "25%    19049.000000    33.202974   -96.580090  7.010000e+02      9.000000   \n",
      "50%    30067.000000    37.910342   -86.708116  1.968000e+03     31.000000   \n",
      "75%    47039.000000    42.181730   -77.358102  7.693500e+03    112.000000   \n",
      "max    99999.000000    71.706900   178.065000  2.636045e+06  81593.000000   \n",
      "\n",
      "          Recovered        Active  Incident_Rate  Case_Fatality_Ratio  \n",
      "count  4.011000e+03  4.011000e+03    3922.000000          3963.000000  \n",
      "mean   1.180731e+04  9.259591e+03    5738.940929             2.228699  \n",
      "std    8.193885e+04  5.719777e+04    3227.374130            17.174874  \n",
      "min    0.000000e+00  0.000000e+00       0.000000             0.000000  \n",
      "25%    0.000000e+00  5.780000e+02    3536.967442             0.969239  \n",
      "50%    0.000000e+00  1.657000e+03    5840.344304             1.551567  \n",
      "75%    0.000000e+00  4.915500e+03    7752.568815             2.341350  \n",
      "max    2.114760e+06  2.400750e+06   27388.219766          1026.206897  \n"
     ]
    }
   ],
   "source": [
    "numeric_summary = df.describe()\n",
    "print(numeric_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5528ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country_Region\n",
      "US          3276\n",
      "Russia        83\n",
      "Japan         49\n",
      "India         37\n",
      "Colombia      34\n",
      "            ... \n",
      "Haiti          1\n",
      "Holy See       1\n",
      "Honduras       1\n",
      "Hungary        1\n",
      "Tuvalu         1\n",
      "Name: count, Length: 200, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "country_counts = df['Country_Region'].value_counts()\n",
    "print(country_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c8febc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4011 entries, 0 to 4010\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   FIPS                 3265 non-null   float64\n",
      " 1   Admin2               3270 non-null   object \n",
      " 2   Province_State       3833 non-null   object \n",
      " 3   Country_Region       4011 non-null   object \n",
      " 4   Last_Update          4011 non-null   object \n",
      " 5   Lat                  3922 non-null   float64\n",
      " 6   Long_                3922 non-null   float64\n",
      " 7   Confirmed            4011 non-null   int64  \n",
      " 8   Deaths               4011 non-null   int64  \n",
      " 9   Recovered            4011 non-null   int64  \n",
      " 10  Active               4011 non-null   int64  \n",
      " 11  Combined_Key         4011 non-null   object \n",
      " 12  Incident_Rate        3922 non-null   float64\n",
      " 13  Case_Fatality_Ratio  3963 non-null   float64\n",
      "dtypes: float64(5), int64(4), object(5)\n",
      "memory usage: 438.8+ KB\n",
      "None\n",
      "Province_State\n",
      "Texas               255\n",
      "Georgia             161\n",
      "Virginia            134\n",
      "Kentucky            121\n",
      "Missouri            117\n",
      "                   ... \n",
      "Telangana             1\n",
      "Tripura               1\n",
      "Uttar Pradesh         1\n",
      "Uttarakhand           1\n",
      "Pitcairn Islands      1\n",
      "Name: count, Length: 596, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get information on all columns\n",
    "df_info = df.info()\n",
    "print(df_info)\n",
    "\n",
    "# Value counts for non-numeric columns\n",
    "province_counts = df['Province_State'].value_counts()\n",
    "print(province_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f46a8",
   "metadata": {},
   "source": [
    "**Summary of Interaction**\n",
    "\n",
    "Question:\n",
    "\n",
    "The user asked how to provide simple summaries of columns in a dataset available at a specific URL.\n",
    "Response:\n",
    "\n",
    "I explained how to use the pandas library in Python to summarize the dataset. The methods mentioned were:\n",
    "Loading the Dataset: Using pd.read_csv() to read the dataset from the provided URL.\n",
    "Basic Summary: Using df.describe() to get descriptive statistics for numeric columns.\n",
    "Categorical Columns Summary: Using df['column'].value_counts() to see the distribution of values for categorical columns.\n",
    "Non-Numeric Columns: Using df.info() for an overview of data types and value_counts() for non-numeric columns.\n",
    "\n",
    "https://chatgpt.com/share/baf8d96b-a9d1-4471-ba61-1ab751eea509"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0c764",
   "metadata": {},
   "source": [
    "<h3>4. If the dataset you're using has (a) non-numeric variables and (b) missing values in numeric variables, explain (perhaps using help from a ChatBot if needed) the discrepancies between size of the dataset given by df.shape and what is reported by df.describe() with respect to (a) the number of columns it analyzes and (b) the values it reports in the \"count\" column<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9358deb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "      <th>Incident_Rate</th>\n",
       "      <th>Case_Fatality_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3265.000000</td>\n",
       "      <td>3270</td>\n",
       "      <td>3833</td>\n",
       "      <td>4011</td>\n",
       "      <td>4011</td>\n",
       "      <td>3922.000000</td>\n",
       "      <td>3922.000000</td>\n",
       "      <td>4.011000e+03</td>\n",
       "      <td>4011.000000</td>\n",
       "      <td>4.011000e+03</td>\n",
       "      <td>4.011000e+03</td>\n",
       "      <td>4011</td>\n",
       "      <td>3922.000000</td>\n",
       "      <td>3963.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1925</td>\n",
       "      <td>596</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Unassigned</td>\n",
       "      <td>Texas</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>255</td>\n",
       "      <td>3276</td>\n",
       "      <td>3995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32386.688208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.768908</td>\n",
       "      <td>-71.113714</td>\n",
       "      <td>2.102613e+04</td>\n",
       "      <td>476.518823</td>\n",
       "      <td>1.180731e+04</td>\n",
       "      <td>9.259591e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5738.940929</td>\n",
       "      <td>2.228699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18017.294540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.391261</td>\n",
       "      <td>55.220345</td>\n",
       "      <td>1.055172e+05</td>\n",
       "      <td>2833.090464</td>\n",
       "      <td>8.193885e+04</td>\n",
       "      <td>5.719777e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3227.374130</td>\n",
       "      <td>17.174874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-71.949900</td>\n",
       "      <td>-175.198200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19049.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.202974</td>\n",
       "      <td>-96.580090</td>\n",
       "      <td>7.010000e+02</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.780000e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3536.967442</td>\n",
       "      <td>0.969239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30067.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.910342</td>\n",
       "      <td>-86.708116</td>\n",
       "      <td>1.968000e+03</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.657000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5840.344304</td>\n",
       "      <td>1.551567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47039.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.181730</td>\n",
       "      <td>-77.358102</td>\n",
       "      <td>7.693500e+03</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.915500e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7752.568815</td>\n",
       "      <td>2.341350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.706900</td>\n",
       "      <td>178.065000</td>\n",
       "      <td>2.636045e+06</td>\n",
       "      <td>81593.000000</td>\n",
       "      <td>2.114760e+06</td>\n",
       "      <td>2.400750e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27388.219766</td>\n",
       "      <td>1026.206897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                FIPS      Admin2 Province_State Country_Region  \\\n",
       "count    3265.000000        3270           3833           4011   \n",
       "unique           NaN        1925            596            200   \n",
       "top              NaN  Unassigned          Texas             US   \n",
       "freq             NaN          51            255           3276   \n",
       "mean    32386.688208         NaN            NaN            NaN   \n",
       "std     18017.294540         NaN            NaN            NaN   \n",
       "min        66.000000         NaN            NaN            NaN   \n",
       "25%     19049.000000         NaN            NaN            NaN   \n",
       "50%     30067.000000         NaN            NaN            NaN   \n",
       "75%     47039.000000         NaN            NaN            NaN   \n",
       "max     99999.000000         NaN            NaN            NaN   \n",
       "\n",
       "                Last_Update          Lat        Long_     Confirmed  \\\n",
       "count                  4011  3922.000000  3922.000000  4.011000e+03   \n",
       "unique                    6          NaN          NaN           NaN   \n",
       "top     2021-01-02 05:22:33          NaN          NaN           NaN   \n",
       "freq                   3995          NaN          NaN           NaN   \n",
       "mean                    NaN    35.768908   -71.113714  2.102613e+04   \n",
       "std                     NaN    13.391261    55.220345  1.055172e+05   \n",
       "min                     NaN   -71.949900  -175.198200  0.000000e+00   \n",
       "25%                     NaN    33.202974   -96.580090  7.010000e+02   \n",
       "50%                     NaN    37.910342   -86.708116  1.968000e+03   \n",
       "75%                     NaN    42.181730   -77.358102  7.693500e+03   \n",
       "max                     NaN    71.706900   178.065000  2.636045e+06   \n",
       "\n",
       "              Deaths     Recovered        Active Combined_Key  Incident_Rate  \\\n",
       "count    4011.000000  4.011000e+03  4.011000e+03         4011    3922.000000   \n",
       "unique           NaN           NaN           NaN         4011            NaN   \n",
       "top              NaN           NaN           NaN  Afghanistan            NaN   \n",
       "freq             NaN           NaN           NaN            1            NaN   \n",
       "mean      476.518823  1.180731e+04  9.259591e+03          NaN    5738.940929   \n",
       "std      2833.090464  8.193885e+04  5.719777e+04          NaN    3227.374130   \n",
       "min         0.000000  0.000000e+00  0.000000e+00          NaN       0.000000   \n",
       "25%         9.000000  0.000000e+00  5.780000e+02          NaN    3536.967442   \n",
       "50%        31.000000  0.000000e+00  1.657000e+03          NaN    5840.344304   \n",
       "75%       112.000000  0.000000e+00  4.915500e+03          NaN    7752.568815   \n",
       "max     81593.000000  2.114760e+06  2.400750e+06          NaN   27388.219766   \n",
       "\n",
       "        Case_Fatality_Ratio  \n",
       "count           3963.000000  \n",
       "unique                  NaN  \n",
       "top                     NaN  \n",
       "freq                    NaN  \n",
       "mean               2.228699  \n",
       "std               17.174874  \n",
       "min                0.000000  \n",
       "25%                0.969239  \n",
       "50%                1.551567  \n",
       "75%                2.341350  \n",
       "max             1026.206897  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e63213",
   "metadata": {},
   "source": [
    "The output from df.shape was (4011, 14), which represents the number of rows and columns. However, df.describe only provided statistics for 9 columns, indicating that 5 columns were missing. By definition, df.shape shows the number of rows and columns in the DataFrame, while df.describe() gives a statistical summary of the DataFrame. According to the Chatbot, the discrepancies are caused by columns that are non-numeric or have mixed types (e.g., strings, dates, etc.). Columns where all data values are missing or where statistics are not applicable are not included in df.describe. Upon further investigation, we noted that previously in question number 3, we identified all the data types of each column, confirming that the 5 columns missing from df.describe have a data type of object. By default, df.describe provides summary statistics only for numerical data. To include all data types, we need to use df.describe(include='all')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fccff4c",
   "metadata": {},
   "source": [
    "**Summary of Interaction**\n",
    "User Inquiry: The user sought an explanation for discrepancies between the dataset size reported by df.shape and what is reported by df.describe() with respect to the number of columns analyzed and the values in the \"count\" column.\n",
    "\n",
    "Details Provided:\n",
    "\n",
    "df.shape Output: (4011, 14) indicating the DataFrame has 4011 rows and 14 columns.\n",
    "df.describe() Output: Provided summary statistics for selected columns including FIPS, Lat, Long_, Confirmed, Deaths, Recovered, Active, Incident_Rate, Case_Fatality_Ratio, with varying counts reflecting missing values.\n",
    "Analysis Provided:\n",
    "\n",
    "Number of Columns Analyzed:\n",
    "\n",
    "df.describe() by default processes only numeric columns. Non-numeric columns (e.g., strings, dates) are excluded from the summary. This is why some columns are not included in the df.describe() output.\n",
    "Count Values:\n",
    "\n",
    "The count values in df.describe() reflect the number of non-null entries for each numeric column. Discrepancies in counts are due to missing values in some columns.\n",
    "Additional Details:\n",
    "\n",
    "Data Types and Summary:\n",
    "\n",
    "df.describe() does not include non-numeric columns by default. For a complete summary, including non-numeric columns, the include parameter can be used:\n",
    "df.describe(include='all') provides summaries for all columns, including non-numeric ones.\n",
    "df.describe(include=[object, 'number']) allows for specific types to be included in the summary.\n",
    "Example Provided:\n",
    "\n",
    "An example was given of using df.describe(include='all') to include summaries for both numeric and non-numeric columns.\n",
    "User's Data Details:\n",
    "\n",
    "Column Overview:\n",
    "FIPS: 3265 non-null, float64\n",
    "Admin2: 3270 non-null, object\n",
    "Province_State: 3833 non-null, object\n",
    "Country_Region: 4011 non-null, object\n",
    "Last_Update: 4011 non-null, object\n",
    "Lat: 3922 non-null, float64\n",
    "Long_: 3922 non-null, float64\n",
    "Confirmed: 4011 non-null, int64\n",
    "Deaths: 4011 non-null, int64\n",
    "Recovered: 4011 non-null, int64\n",
    "Active: 4011 non-null, int64\n",
    "Combined_Key: 4011 non-null, object\n",
    "Incident_Rate: 3922 non-null, float64\n",
    "Case_Fatality_Ratio: 3963 non-null, float64\n",
    "Conclusion:\n",
    "\n",
    "df.describe() provides summaries only for numeric columns by default. To include all columns in the summary, use the include parameter. Discrepancies in the count values are due to missing values in some columns.\n",
    "\n",
    "https://chatgpt.com/share/1a6da042-771b-40dc-afea-a9e4f27b40b8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3b5ec",
   "metadata": {},
   "source": [
    "<h3>5. Use your ChatBot session to help understand the difference between the following and then provide your own paraphrasing summarization of that difference<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07bb7c",
   "metadata": {},
   "source": [
    "An attribute is a variable associated with an object, while a method is a function associated with an object. The reason an attribute does not end with parentheses, unlike a method, is because an attribute does not require arguments, whereas a method may need arguments. Attributes do not perform any operations; they only hold data. On the other hand, methods can perform computations, modify the object's state, or even return new objects. For example, df.shape is an attribute of an object called df, which accesses the data about df's structure, specifically the number of rows and columns. In contrast, df.describe() is a method of df that, when called, performs calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c5876",
   "metadata": {},
   "source": [
    "**Summary of Interaction on Attributes and Methods**\n",
    "Introduction to Attributes and Methods:\n",
    "\n",
    "Attributes: We discussed that attributes are properties or variables associated with an object in Python. They store data about the object and are accessed directly without parentheses. An example given was df.shape, which provides the dimensions of a DataFrame (e.g., (3, 2) for 3 rows and 2 columns).\n",
    "Methods: Methods are functions associated with an object that can perform operations using the object's data. They are called with parentheses, which may include arguments. An example provided was df.describe(), which generates descriptive statistics of the DataFrame.\n",
    "Key Differences:\n",
    "\n",
    "Functionality:\n",
    "Attributes are like variables that hold data.\n",
    "Methods are like functions that perform actions or computations.\n",
    "Syntax:\n",
    "Attributes are accessed with object.attribute (no parentheses).\n",
    "Methods are called with object.method() (with parentheses).\n",
    "Purpose:\n",
    "Attributes store or retrieve data.\n",
    "Methods perform operations, often using the data stored in attributes.\n",
    "Conceptual Analogy:\n",
    "\n",
    "We concluded that attributes are analogous to nouns (describing something about the object), while methods are analogous to verbs (performing actions related to the object).\n",
    "\n",
    "https://chatgpt.com/share/355a7391-c35f-4245-8959-d008d0d4879c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2fcb7",
   "metadata": {},
   "source": [
    "<h1>\"Post-lecture\" HW [submission along with \"Pre-lecture\" HW is due prior to next TUT]<h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f9e7d",
   "metadata": {},
   "source": [
    "<h3>6. The df.describe() method provides the 'count', 'mean', 'std', 'min', '25%', '50%', '75%', and 'max' summary statistics for each variable it analyzes. Give the definitions (perhaps using help from the ChatBot if needed) of each of these summary statistics<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4b7cf",
   "metadata": {},
   "source": [
    "**Count** refers to the total number of non-null data points present in the column.\n",
    "\n",
    "**Mean** is the average value in the column. It is calculated by summing all the values and dividing by the number of data points.\n",
    "\n",
    "**Std** stands for Standard Deviation, which indicates the amount of variation or dispersion in the data. A higher standard deviation means the values are more spread out from the mean, while a lower standard deviation means they are closer to the mean.\n",
    "\n",
    "**Min** represents the smallest data point in the column, or the minimum value.\n",
    "\n",
    "**25%** refers to the 25th percentile, also known as the first quartile. This means that 25% of the data is less than this value.\n",
    "\n",
    "**50%** refers to the 50th percentile, also known as the median. This indicates that 50% of the data is less than this value.\n",
    "**75%** refers to the 75th percentile, also known as the third quartile. This means that 75% of the data is less than this value.\n",
    "\n",
    "**Max** represents the largest data point in the column, or the maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b69646",
   "metadata": {},
   "source": [
    "**Summary of Discussion** on Descriptive Statistics in Pandas (df.describe())\n",
    "\n",
    "We explored various statistical terms generated by the df.describe() function in Python's pandas library, which provides an overview of numerical columns in a DataFrame.\n",
    "\n",
    "Count:\n",
    "\n",
    "Represents the number of non-null (non-missing) values in the column. It's useful for understanding data completeness and determining how many values are available for analysis.\n",
    "Mean:\n",
    "\n",
    "The average of the data values, calculated by summing all values and dividing by the number of values. It shows the central tendency of the data, though it's sensitive to outliers.\n",
    "Standard Deviation (std):\n",
    "\n",
    "Measures the spread of the data from the mean. A higher standard deviation indicates greater variability, while a lower value suggests that the data points are closer to the mean. It is calculated using the squared differences from the mean and taking the square root.\n",
    "Min:\n",
    "\n",
    "The smallest value in the column, indicating the lower bound of the data.\n",
    "Percentiles (25%, 50%, 75%):\n",
    "\n",
    "Percentiles help understand the distribution of data:\n",
    "25% (Q1): The value below which 25% of the data falls.\n",
    "50% (Median): The value below which 50% of the data falls, representing the middle of the dataset.\n",
    "75% (Q3): The value below which 75% of the data falls, providing insight into the upper portion of the data.\n",
    "These percentiles help identify the spread and distribution of values in a dataset.\n",
    "Max:\n",
    "\n",
    "The largest value in the dataset, representing the upper boundary of the data. It helps define the range when paired with the Min, but may be influenced by outliers.\n",
    "\n",
    "https://chatgpt.com/share/8350bfca-5105-475b-946a-1769742bfc29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef40ccf",
   "metadata": {},
   "source": [
    "<h3>7. Missing data can be considered \"across rows\" or \"down columns\". Consider how df.dropna() or del df['col'] should be applied to most efficiently use the available non-missing data in your dataset and briefly answer the following questions in your own words<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78590cd",
   "metadata": {},
   "source": [
    "<h4>1. Provide an example of a \"use case\" in which using df.dropna() might be peferred over using del df['col'] <h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6556c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name   Age     City\n",
      "0  Darren  12.0  Jakarta\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Name': ['Darren', 'Tara', None, None],\n",
    "        'Age': [12, None, 1, 7],\n",
    "        'City': ['Jakarta', 'Bandung', None, 'Korea']}\n",
    "\n",
    "df_2 = pd.DataFrame(data)\n",
    "df_cleaned = df_2.dropna()\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed5da5",
   "metadata": {},
   "source": [
    "df.dropna() removes all rows that contain a missing value (NaN). In the example above, using df.dropna() is more suitable and convenient because it automatically identifies rows with missing values and drops them directly. This approach is preferred over using del df['col'], as it efficiently handles missing data without the need to manually specify which columns to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af44c375",
   "metadata": {},
   "source": [
    "<h4>2. Provide an example of \"the opposite use case\" in which using del df['col'] might be preferred over using df.dropna()<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b84143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after using del to drop 'Salary' column:\n",
      "     Name   Age         City\n",
      "0  Darren  25.0     New York\n",
      "1   Putra   NaN  Los Angeles\n",
      "2  Rasyid  30.0         None\n",
      "3    Kont  40.0      Chicago\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with missing values\n",
    "data = {'Name': ['Darren', 'Putra', 'Rasyid', 'Kont'],\n",
    "        'Age': [25, None, 30, 40],\n",
    "        'City': ['New York', 'Los Angeles', None, 'Chicago'],\n",
    "        'Salary': [None, None, None, None]}  # Column with all NaN values\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "del df['Salary']\n",
    "\n",
    "print(\"\\nDataFrame after using del to drop 'Salary' column:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0b1f4",
   "metadata": {},
   "source": [
    "Using del df['col'] removes an entire column from the DataFrame. In this case, del df['col'] is more appropriate than using df.dropna(), as the latter would remove all rows with any missing values. We can see that the 'salary' column contains no data, with all values missing. Clearly, the 'salary' column is irrelevant due to the lack of data. The del df['col'] operation is useful when you want to permanently discard specific columns, either because they are unnecessary or too incomplete to be of value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad01ee",
   "metadata": {},
   "source": [
    "<h4>3. Discuss why applying del df['col'] before df.dropna() when both are used together could be important<h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b13d5",
   "metadata": {},
   "source": [
    "Applying del df['col'] before df.dropna() when used together is very important. It can help prevent data loss. If a column that we want to delete contains NaN values, removing the column first ensures that rows which actually do not have missing values are preserved. This way, we retain more data by preventing the inadvertent loss of useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0c8a3",
   "metadata": {},
   "source": [
    "<h4>4. Remove all missing data from one of the datasets you're considering using some combination of del df['col'] and/or df.dropna() and give a justification for your approach, including a \"before and after\" report of the results of your approach for your dataset.\n",
    "<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512b5150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "      <th>Incident_Rate</th>\n",
       "      <th>Case_Fatality_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>33.93911</td>\n",
       "      <td>67.709953</td>\n",
       "      <td>52513</td>\n",
       "      <td>2201</td>\n",
       "      <td>41727</td>\n",
       "      <td>8585</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>134.896578</td>\n",
       "      <td>4.191343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>41.15330</td>\n",
       "      <td>20.168300</td>\n",
       "      <td>58316</td>\n",
       "      <td>1181</td>\n",
       "      <td>33634</td>\n",
       "      <td>23501</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2026.409062</td>\n",
       "      <td>2.025173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>28.03390</td>\n",
       "      <td>1.659600</td>\n",
       "      <td>99897</td>\n",
       "      <td>2762</td>\n",
       "      <td>67395</td>\n",
       "      <td>29740</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>227.809861</td>\n",
       "      <td>2.764848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>42.50630</td>\n",
       "      <td>1.521800</td>\n",
       "      <td>8117</td>\n",
       "      <td>84</td>\n",
       "      <td>7463</td>\n",
       "      <td>570</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>10505.403482</td>\n",
       "      <td>1.034865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Angola</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-11.20270</td>\n",
       "      <td>17.873900</td>\n",
       "      <td>17568</td>\n",
       "      <td>405</td>\n",
       "      <td>11146</td>\n",
       "      <td>6017</td>\n",
       "      <td>Angola</td>\n",
       "      <td>53.452981</td>\n",
       "      <td>2.305328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown, Ukraine</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nauru</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-0.52280</td>\n",
       "      <td>166.931500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nauru</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Niue</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-19.05440</td>\n",
       "      <td>-169.867200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Niue, New Zealand</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tuvalu</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-7.10950</td>\n",
       "      <td>177.649300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tuvalu</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pitcairn Islands</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-24.37680</td>\n",
       "      <td>-128.324200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pitcairn Islands, United Kingdom</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4011 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FIPS Admin2    Province_State  Country_Region          Last_Update  \\\n",
       "0      NaN    NaN               NaN     Afghanistan  2021-01-02 05:22:33   \n",
       "1      NaN    NaN               NaN         Albania  2021-01-02 05:22:33   \n",
       "2      NaN    NaN               NaN         Algeria  2021-01-02 05:22:33   \n",
       "3      NaN    NaN               NaN         Andorra  2021-01-02 05:22:33   \n",
       "4      NaN    NaN               NaN          Angola  2021-01-02 05:22:33   \n",
       "...    ...    ...               ...             ...                  ...   \n",
       "4006   NaN    NaN           Unknown         Ukraine  2021-01-02 05:22:33   \n",
       "4007   NaN    NaN               NaN           Nauru  2021-01-02 05:22:33   \n",
       "4008   NaN    NaN              Niue     New Zealand  2021-01-02 05:22:33   \n",
       "4009   NaN    NaN               NaN          Tuvalu  2021-01-02 05:22:33   \n",
       "4010   NaN    NaN  Pitcairn Islands  United Kingdom  2021-01-02 05:22:33   \n",
       "\n",
       "           Lat       Long_  Confirmed  Deaths  Recovered  Active  \\\n",
       "0     33.93911   67.709953      52513    2201      41727    8585   \n",
       "1     41.15330   20.168300      58316    1181      33634   23501   \n",
       "2     28.03390    1.659600      99897    2762      67395   29740   \n",
       "3     42.50630    1.521800       8117      84       7463     570   \n",
       "4    -11.20270   17.873900      17568     405      11146    6017   \n",
       "...        ...         ...        ...     ...        ...     ...   \n",
       "4006       NaN         NaN          0       0          0       0   \n",
       "4007  -0.52280  166.931500          0       0          0       0   \n",
       "4008 -19.05440 -169.867200          0       0          0       0   \n",
       "4009  -7.10950  177.649300          0       0          0       0   \n",
       "4010 -24.37680 -128.324200          0       0          0       0   \n",
       "\n",
       "                          Combined_Key  Incident_Rate  Case_Fatality_Ratio  \n",
       "0                          Afghanistan     134.896578             4.191343  \n",
       "1                              Albania    2026.409062             2.025173  \n",
       "2                              Algeria     227.809861             2.764848  \n",
       "3                              Andorra   10505.403482             1.034865  \n",
       "4                               Angola      53.452981             2.305328  \n",
       "...                                ...            ...                  ...  \n",
       "4006                  Unknown, Ukraine       0.000000             0.000000  \n",
       "4007                             Nauru       0.000000             0.000000  \n",
       "4008                 Niue, New Zealand       0.000000             0.000000  \n",
       "4009                            Tuvalu       0.000000             0.000000  \n",
       "4010  Pitcairn Islands, United Kingdom       0.000000             0.000000  \n",
       "\n",
       "[4011 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537415bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "      <th>Incident_Rate</th>\n",
       "      <th>Case_Fatality_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>32.539527</td>\n",
       "      <td>-86.644082</td>\n",
       "      <td>4239</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>4189</td>\n",
       "      <td>Autauga, Alabama, US</td>\n",
       "      <td>7587.391935</td>\n",
       "      <td>1.179523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>1003.0</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>30.727750</td>\n",
       "      <td>-87.722071</td>\n",
       "      <td>13823</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>13654</td>\n",
       "      <td>Baldwin, Alabama, US</td>\n",
       "      <td>6192.157109</td>\n",
       "      <td>1.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>1005.0</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>31.868263</td>\n",
       "      <td>-85.387129</td>\n",
       "      <td>1517</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1484</td>\n",
       "      <td>Barbour, Alabama, US</td>\n",
       "      <td>6145.183505</td>\n",
       "      <td>2.175346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>1007.0</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>32.996421</td>\n",
       "      <td>-87.125115</td>\n",
       "      <td>1854</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>1808</td>\n",
       "      <td>Bibb, Alabama, US</td>\n",
       "      <td>8279.003304</td>\n",
       "      <td>2.481122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>1009.0</td>\n",
       "      <td>Blount</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>33.982109</td>\n",
       "      <td>-86.567906</td>\n",
       "      <td>4693</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>4630</td>\n",
       "      <td>Blount, Alabama, US</td>\n",
       "      <td>8115.726490</td>\n",
       "      <td>1.342425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>56041.0</td>\n",
       "      <td>Uinta</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>41.287818</td>\n",
       "      <td>-110.547578</td>\n",
       "      <td>1558</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1551</td>\n",
       "      <td>Uinta, Wyoming, US</td>\n",
       "      <td>7702.956591</td>\n",
       "      <td>0.449294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>56043.0</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>43.904516</td>\n",
       "      <td>-107.680187</td>\n",
       "      <td>781</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>762</td>\n",
       "      <td>Washakie, Wyoming, US</td>\n",
       "      <td>10006.406150</td>\n",
       "      <td>2.432778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>56045.0</td>\n",
       "      <td>Weston</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>43.839612</td>\n",
       "      <td>-104.567488</td>\n",
       "      <td>476</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>474</td>\n",
       "      <td>Weston, Wyoming, US</td>\n",
       "      <td>6871.661614</td>\n",
       "      <td>0.420168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>2066.0</td>\n",
       "      <td>Copper River</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-04-02 15:13:53</td>\n",
       "      <td>60.388600</td>\n",
       "      <td>-162.890520</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>Copper River, Alaska, US</td>\n",
       "      <td>5816.969248</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>2063.0</td>\n",
       "      <td>Chugach</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-04-02 15:13:53</td>\n",
       "      <td>61.166660</td>\n",
       "      <td>-149.900000</td>\n",
       "      <td>221</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>218</td>\n",
       "      <td>Chugach, Alaska, US</td>\n",
       "      <td>3273.589098</td>\n",
       "      <td>1.357466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3194 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FIPS        Admin2 Province_State Country_Region  \\\n",
       "648    1001.0       Autauga        Alabama             US   \n",
       "649    1003.0       Baldwin        Alabama             US   \n",
       "650    1005.0       Barbour        Alabama             US   \n",
       "651    1007.0          Bibb        Alabama             US   \n",
       "652    1009.0        Blount        Alabama             US   \n",
       "...       ...           ...            ...            ...   \n",
       "3918  56041.0         Uinta        Wyoming             US   \n",
       "3920  56043.0      Washakie        Wyoming             US   \n",
       "3921  56045.0        Weston        Wyoming             US   \n",
       "3976   2066.0  Copper River         Alaska             US   \n",
       "3977   2063.0       Chugach         Alaska             US   \n",
       "\n",
       "              Last_Update        Lat       Long_  Confirmed  Deaths  \\\n",
       "648   2021-01-02 05:22:33  32.539527  -86.644082       4239      50   \n",
       "649   2021-01-02 05:22:33  30.727750  -87.722071      13823     169   \n",
       "650   2021-01-02 05:22:33  31.868263  -85.387129       1517      33   \n",
       "651   2021-01-02 05:22:33  32.996421  -87.125115       1854      46   \n",
       "652   2021-01-02 05:22:33  33.982109  -86.567906       4693      63   \n",
       "...                   ...        ...         ...        ...     ...   \n",
       "3918  2021-01-02 05:22:33  41.287818 -110.547578       1558       7   \n",
       "3920  2021-01-02 05:22:33  43.904516 -107.680187        781      19   \n",
       "3921  2021-01-02 05:22:33  43.839612 -104.567488        476       2   \n",
       "3976  2021-04-02 15:13:53  60.388600 -162.890520        157       0   \n",
       "3977  2021-04-02 15:13:53  61.166660 -149.900000        221       3   \n",
       "\n",
       "      Recovered  Active              Combined_Key  Incident_Rate  \\\n",
       "648           0    4189      Autauga, Alabama, US    7587.391935   \n",
       "649           0   13654      Baldwin, Alabama, US    6192.157109   \n",
       "650           0    1484      Barbour, Alabama, US    6145.183505   \n",
       "651           0    1808         Bibb, Alabama, US    8279.003304   \n",
       "652           0    4630       Blount, Alabama, US    8115.726490   \n",
       "...         ...     ...                       ...            ...   \n",
       "3918          0    1551        Uinta, Wyoming, US    7702.956591   \n",
       "3920          0     762     Washakie, Wyoming, US   10006.406150   \n",
       "3921          0     474       Weston, Wyoming, US    6871.661614   \n",
       "3976          0     157  Copper River, Alaska, US    5816.969248   \n",
       "3977          0     218       Chugach, Alaska, US    3273.589098   \n",
       "\n",
       "      Case_Fatality_Ratio  \n",
       "648              1.179523  \n",
       "649              1.222600  \n",
       "650              2.175346  \n",
       "651              2.481122  \n",
       "652              1.342425  \n",
       "...                   ...  \n",
       "3918             0.449294  \n",
       "3920             2.432778  \n",
       "3921             0.420168  \n",
       "3976             0.000000  \n",
       "3977             1.357466  \n",
       "\n",
       "[3194 rows x 14 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17017379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIPS                   746\n",
       "Admin2                 741\n",
       "Province_State         178\n",
       "Country_Region           0\n",
       "Last_Update              0\n",
       "Lat                     89\n",
       "Long_                   89\n",
       "Confirmed                0\n",
       "Deaths                   0\n",
       "Recovered                0\n",
       "Active                   0\n",
       "Combined_Key             0\n",
       "Incident_Rate           89\n",
       "Case_Fatality_Ratio     48\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d712d5",
   "metadata": {},
   "source": [
    "We can see that column FIPS and Admin2 have the most missing value. Where if we try to drop both of the columns first and then drop all rows that has missing value. The result would be like this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02193d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "      <th>Incident_Rate</th>\n",
       "      <th>Case_Fatality_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Australian Capital Territory</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-35.473500</td>\n",
       "      <td>149.01240</td>\n",
       "      <td>118</td>\n",
       "      <td>3</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>Australian Capital Territory, Australia</td>\n",
       "      <td>27.563653</td>\n",
       "      <td>2.542373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-33.868800</td>\n",
       "      <td>151.20930</td>\n",
       "      <td>4947</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>4893</td>\n",
       "      <td>New South Wales, Australia</td>\n",
       "      <td>60.938655</td>\n",
       "      <td>1.091571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Northern Territory</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-12.463400</td>\n",
       "      <td>130.84560</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>Northern Territory, Australia</td>\n",
       "      <td>30.537459</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Queensland</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-27.469800</td>\n",
       "      <td>153.02510</td>\n",
       "      <td>1255</td>\n",
       "      <td>6</td>\n",
       "      <td>1224</td>\n",
       "      <td>25</td>\n",
       "      <td>Queensland, Australia</td>\n",
       "      <td>24.533281</td>\n",
       "      <td>0.478088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>South Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-34.928500</td>\n",
       "      <td>138.60070</td>\n",
       "      <td>580</td>\n",
       "      <td>4</td>\n",
       "      <td>566</td>\n",
       "      <td>10</td>\n",
       "      <td>South Australia, Australia</td>\n",
       "      <td>33.020211</td>\n",
       "      <td>0.689655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>W.P. Putrajaya</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>2.926400</td>\n",
       "      <td>101.69640</td>\n",
       "      <td>285</td>\n",
       "      <td>3</td>\n",
       "      <td>244</td>\n",
       "      <td>38</td>\n",
       "      <td>W.P. Putrajaya, Malaysia</td>\n",
       "      <td>270.398482</td>\n",
       "      <td>1.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>Jersey</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>49.213800</td>\n",
       "      <td>-2.13580</td>\n",
       "      <td>2739</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jersey, United Kingdom</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>Guernsey</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>49.448196</td>\n",
       "      <td>-2.58949</td>\n",
       "      <td>298</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Guernsey, United Kingdom</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>Niue</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-19.054400</td>\n",
       "      <td>-169.86720</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Niue, New Zealand</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>Pitcairn Islands</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2021-01-02 05:22:33</td>\n",
       "      <td>-24.376800</td>\n",
       "      <td>-128.32420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pitcairn Islands, United Kingdom</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3742 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Province_State  Country_Region          Last_Update  \\\n",
       "8     Australian Capital Territory       Australia  2021-01-02 05:22:33   \n",
       "9                  New South Wales       Australia  2021-01-02 05:22:33   \n",
       "10              Northern Territory       Australia  2021-01-02 05:22:33   \n",
       "11                      Queensland       Australia  2021-01-02 05:22:33   \n",
       "12                 South Australia       Australia  2021-01-02 05:22:33   \n",
       "...                            ...             ...                  ...   \n",
       "3998                W.P. Putrajaya        Malaysia  2021-01-02 05:22:33   \n",
       "4003                        Jersey  United Kingdom  2021-01-02 05:22:33   \n",
       "4004                      Guernsey  United Kingdom  2021-01-02 05:22:33   \n",
       "4008                          Niue     New Zealand  2021-01-02 05:22:33   \n",
       "4010              Pitcairn Islands  United Kingdom  2021-01-02 05:22:33   \n",
       "\n",
       "            Lat      Long_  Confirmed  Deaths  Recovered  Active  \\\n",
       "8    -35.473500  149.01240        118       3        114       1   \n",
       "9    -33.868800  151.20930       4947      54          0    4893   \n",
       "10   -12.463400  130.84560         75       0         71       4   \n",
       "11   -27.469800  153.02510       1255       6       1224      25   \n",
       "12   -34.928500  138.60070        580       4        566      10   \n",
       "...         ...        ...        ...     ...        ...     ...   \n",
       "3998   2.926400  101.69640        285       3        244      38   \n",
       "4003  49.213800   -2.13580       2739      42          0       0   \n",
       "4004  49.448196   -2.58949        298      16          0       0   \n",
       "4008 -19.054400 -169.86720          0       0          0       0   \n",
       "4010 -24.376800 -128.32420          0       0          0       0   \n",
       "\n",
       "                                 Combined_Key  Incident_Rate  \\\n",
       "8     Australian Capital Territory, Australia      27.563653   \n",
       "9                  New South Wales, Australia      60.938655   \n",
       "10              Northern Territory, Australia      30.537459   \n",
       "11                      Queensland, Australia      24.533281   \n",
       "12                 South Australia, Australia      33.020211   \n",
       "...                                       ...            ...   \n",
       "3998                 W.P. Putrajaya, Malaysia     270.398482   \n",
       "4003                   Jersey, United Kingdom       0.000000   \n",
       "4004                 Guernsey, United Kingdom       0.000000   \n",
       "4008                        Niue, New Zealand       0.000000   \n",
       "4010         Pitcairn Islands, United Kingdom       0.000000   \n",
       "\n",
       "      Case_Fatality_Ratio  \n",
       "8                2.542373  \n",
       "9                1.091571  \n",
       "10               0.000000  \n",
       "11               0.478088  \n",
       "12               0.689655  \n",
       "...                   ...  \n",
       "3998             1.052632  \n",
       "4003             0.000000  \n",
       "4004             0.000000  \n",
       "4008             0.000000  \n",
       "4010             0.000000  \n",
       "\n",
       "[3742 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df['FIPS']\n",
    "del df['Admin2']\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235eebc",
   "metadata": {},
   "source": [
    "Here we have 3742 rows of data, which mean we have 3742 - 3192 = 550 more data when we delete some of the irelevant column first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc649717",
   "metadata": {},
   "source": [
    "Here's a **summary** of our chat:\n",
    "\n",
    "df.dropna:\n",
    "\n",
    "It's a method in pandas used to remove rows or columns from a DataFrame that contain missing values (NaN).\n",
    "By default, it removes rows (axis=0), but you can also specify axis=1 to remove columns.\n",
    "You can customize its behavior with parameters like how, thresh, subset, and inplace.\n",
    "del df['col']:\n",
    "\n",
    "This command deletes a specific column ('col') from a DataFrame df.\n",
    "It is an in-place operation and directly modifies the DataFrame.\n",
    "Common Use Cases:\n",
    "\n",
    "df.dropna is typically used to drop rows with missing values, but it can also be used to drop columns with missing values depending on the axis parameter.\n",
    "\n",
    "https://chatgpt.com/share/c3ae4b41-2a8e-4afa-8b35-be35d93c498d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdee511",
   "metadata": {},
   "source": [
    " <h3>8. Give brief explanations in your own words for any requested answers to the questions below<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c200f",
   "metadata": {},
   "source": [
    "<h4>1. Use your ChatBot session to understand what df.groupby(\"col1\")[\"col2\"].describe() does and then demonstrate and explain this using a different example from the \"titanic\" data set other than what the ChatBot automatically provide for you<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0875c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
       "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
       "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
       "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
       "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
       "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
       "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
       "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
       "888         0       3  female   NaN      1      2  23.4500        S   Third   \n",
       "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
       "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
       "\n",
       "       who  adult_male deck  embark_town alive  alone  \n",
       "0      man        True  NaN  Southampton    no  False  \n",
       "1    woman       False    C    Cherbourg   yes  False  \n",
       "2    woman       False  NaN  Southampton   yes   True  \n",
       "3    woman       False    C  Southampton   yes  False  \n",
       "4      man        True  NaN  Southampton    no   True  \n",
       "..     ...         ...  ...          ...   ...    ...  \n",
       "886    man        True  NaN  Southampton    no   True  \n",
       "887  woman       False    B  Southampton   yes   True  \n",
       "888  woman       False  NaN  Southampton    no  False  \n",
       "889    man        True    C    Cherbourg   yes   True  \n",
       "890    man        True  NaN   Queenstown    no   True  \n",
       "\n",
       "[891 rows x 15 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\")\n",
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6885d6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>314.0</td>\n",
       "      <td>0.742038</td>\n",
       "      <td>0.438211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>577.0</td>\n",
       "      <td>0.188908</td>\n",
       "      <td>0.391775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std  min  25%  50%  75%  max\n",
       "sex                                                       \n",
       "female  314.0  0.742038  0.438211  0.0  0.0  1.0  1.0  1.0\n",
       "male    577.0  0.188908  0.391775  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.groupby(\"sex\")[\"survived\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34cdf0",
   "metadata": {},
   "source": [
    "By using df_c.groupby(\"sex\")[\"survived\"], i can access the data and group them by sex and manipulate data based on the survived column and see the conection between both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d66d1",
   "metadata": {},
   "source": [
    "<h4>2. Assuming you've not yet removed missing values in the manner of question \"7\" above, df.describe() would have different values in the count value for different data columns depending on the missingness present in the original data. Why do these capture something fundamentally different from the values in the count that result from doing something like df.groupby(\"col1\")[\"col2\"].describe()?\n",
    "<h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5107c6c",
   "metadata": {},
   "source": [
    "When you use `df.describe()`, the count shows how many non-missing values there are in each column across the entire DataFrame, so it tells you about the completeness of data in each column overall. This count can be different if there are missing values in some columns. On the other hand, if you do `df.groupby(\"col1\")[\"col2\"].describe()`, the count shows the number of non-missing values in `col2` within each group defined by `col1`. So, this count can change depending on how the data is distributed in each group. Basically, `df.describe()` gives you a general idea of missing values in the whole dataset, while `df.groupby(\"col1\")[\"col2\"].describe()` breaks it down and shows how missing values vary in different groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08b21b",
   "metadata": {},
   "source": [
    "<h4>3. Intentionally introduce the following errors into your code and report your opinion as to whether it's easier to (a) work in a ChatBot session to fix the errors, or (b) use google to search for and fix errors: first share the errors you get in the ChatBot session and see if you can work with ChatBot to troubleshoot and fix the coding errors, and then see if you think a google search for the error provides the necessary toubleshooting help more quickly than ChatGPT <h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92dcf27",
   "metadata": {},
   "source": [
    "In my opinion, it is far easier to use chatbot sessions like ChatGPT or Copilot from Microsoft to fix errors compared to Google. With a chatbot, I can directly copy and paste the error, and if I don’t understand what the error is about, I can simply ask the chatbot to explain it to me or give me clues on fixing it. I can modify my question to either get a clue on how to fix it or ask the chatbot to fix it directly. And if I don’t understand the code provided by the chatbot, I can easily ask the chatbot to explain it again or request an explanation in simpler terms to make it easier to understand. On the other hand, using Google to find a solution to my error is also possible, but it doesn’t have a feature to ask follow-up questions like a chatbot and have a conversation. Moreover, I need to manually search through websites to find the solution I need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685dbd10",
   "metadata": {},
   "source": [
    "**A. Forget to include import pandas as pd in your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ff0827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_daily_reports/01-01-2021.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      4\u001b[0m missing_values \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(missing_values)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_daily_reports/01-01-2021.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "missing_values = df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578c643",
   "metadata": {},
   "source": [
    "**B. Mistype \"titanic.csv\" as \"titanics.csv\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa3ca27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ur' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_daily_reports/01-01-2021.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mur\u001b[49m)\n\u001b[1;32m      5\u001b[0m missing_values \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(missing_values)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ur' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_daily_reports/01-01-2021.csv\"\n",
    "df = pd.read_csv(ur)\n",
    "\n",
    "missing_values = df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea94d89",
   "metadata": {},
   "source": [
    "**C. Try to use a dataframe before it's been assigned into the variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05062f52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDF\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb827d07",
   "metadata": {},
   "source": [
    "**D. Forget one of the parentheses somewhere the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16091a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (235428196.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.read_csv(ur\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(ur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ad87c",
   "metadata": {},
   "source": [
    "**E. Mistype one of the names of the chained functions with the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19821fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mgroup_by(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.group_by(\"col1\")[\"col2\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d84120",
   "metadata": {},
   "source": [
    "**F. Use a column name that's not in your data for the groupby and column selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b34ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_c \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sex'"
     ]
    }
   ],
   "source": [
    "df_c = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\")\n",
    "df_c.groupby(\"Sex\")[\"survived\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2004635",
   "metadata": {},
   "source": [
    "**G. Forget to put the column name as a string in quotes for the groupby and column selection, and see if the ChatBot and google are still as helpful as they were for the previous question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0677b4af",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_c \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[survived]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sex'"
     ]
    }
   ],
   "source": [
    "df_c = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\")\n",
    "df_c.groupby(\"Sex\")[survived].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91af07a",
   "metadata": {},
   "source": [
    "**summary for part 1**:\n",
    "\n",
    "df.groupby(\"col1\")[\"col2\"].describe(): Provides descriptive statistics for col2 within each group defined by col1. It shows how col2 values are distributed across different categories.\n",
    "\n",
    "df.describe(): Gives overall statistics for each column in the DataFrame, showing counts of non-null values for each column in the entire DataFrame.\n",
    "\n",
    "Key Difference: df.describe() shows a general overview across the whole DataFrame, while df.groupby(\"col1\")[\"col2\"].describe() provides detailed statistics within each specific group, revealing how missing values and data vary across groups.\n",
    "\n",
    "https://chatgpt.com/share/e0495eb9-6bae-4a0b-8dae-af1e97f34cf4\n",
    "\n",
    "**Summary of Interactions for part 3**\n",
    "Issue 1: NameError with pd\n",
    "\n",
    "Error: NameError: name 'pd' is not defined\n",
    "Solution: Ensure that you import pandas with import pandas as pd before using pd.read_csv().\n",
    "Issue 2: NameError due to typo in URL variable\n",
    "\n",
    "Error: NameError: name 'ur' is not defined\n",
    "Solution: Correct the typo by using url instead of ur in pd.read_csv(url).\n",
    "Issue 3: SyntaxError with incomplete input\n",
    "\n",
    "Error: SyntaxError: incomplete input\n",
    "Solution: Ensure that parentheses and quotation marks are properly closed. Use pd.read_csv(url) correctly.\n",
    "Issue 4: NameError due to undefined DataFrame DF\n",
    "\n",
    "Error: NameError: name 'DF' is not defined\n",
    "Solution: Use the correct variable name df and ensure that df is defined earlier in the code.\n",
    "Issue 5: KeyError with groupby method\n",
    "\n",
    "Error: KeyError: 'Sex'\n",
    "Solution: Verify the column names using df.columns and ensure the correct case and spelling are used. The correct usage should be df.groupby(\"sex\")[\"survived\"].describe().\n",
    "Issue 6: Another KeyError with groupby method\n",
    "\n",
    "Error: KeyError: 'Sex'\n",
    "Solution: Ensure that the column names are correctly referenced and enclosed in quotes. Verify the column names using df_c.columns and correct the code to df_c.groupby(\"sex\")[\"survived\"].describe().\n",
    "\n",
    "https://chatgpt.com/share/24c60ef7-c1c1-48f6-84a5-9ea48c752eb5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ce2d9",
   "metadata": {},
   "source": [
    "<h3>9. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2d634",
   "metadata": {},
   "source": [
    "Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0dfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4998af4",
   "metadata": {},
   "source": [
    "<h1>All summaries and chat session are on below each questions<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbc835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
